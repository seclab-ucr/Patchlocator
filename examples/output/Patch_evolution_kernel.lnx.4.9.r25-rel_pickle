(dp0
S'CVE-2019-10538'
p1
(dp2
S'aftercommits'
p3
(dp4
S'a77c16d4981b'
p5
(dp6
(S'drivers/soc/qcom/icnss.c'
p7
S'wlfw_msa_mem_info_send_sync_msg'
p8
tp9
S'static int wlfw_msa_mem_info_send_sync_msg(void)\n{\n\tint ret;\n\tint i;\n\tstruct wlfw_msa_info_req_msg_v01 req;\n\tstruct wlfw_msa_info_resp_msg_v01 resp;\n\tstruct msg_desc req_desc, resp_desc;\n\tuint64_t max_mapped_addr;\n\n\tif (!penv || !penv->wlfw_clnt)\n\t\treturn -ENODEV;\n\n\ticnss_pr_dbg("Sending MSA mem info, state: 0x%lx\\n", penv->state);\n\n\tmemset(&req, 0, sizeof(req));\n\tmemset(&resp, 0, sizeof(resp));\n\n\treq.msa_addr = penv->msa_pa;\n\treq.size = penv->msa_mem_size;\n\n\treq_desc.max_msg_len = WLFW_MSA_INFO_REQ_MSG_V01_MAX_MSG_LEN;\n\treq_desc.msg_id = QMI_WLFW_MSA_INFO_REQ_V01;\n\treq_desc.ei_array = wlfw_msa_info_req_msg_v01_ei;\n\n\tresp_desc.max_msg_len = WLFW_MSA_INFO_RESP_MSG_V01_MAX_MSG_LEN;\n\tresp_desc.msg_id = QMI_WLFW_MSA_INFO_RESP_V01;\n\tresp_desc.ei_array = wlfw_msa_info_resp_msg_v01_ei;\n\n\tpenv->stats.msa_info_req++;\n\n\tret = qmi_send_req_wait(penv->wlfw_clnt, &req_desc, &req, sizeof(req),\n\t\t\t&resp_desc, &resp, sizeof(resp), WLFW_TIMEOUT_MS);\n\tif (ret < 0) {\n\t\ticnss_pr_err("Send MSA Mem info req failed %d\\n", ret);\n\t\tgoto out;\n\t}\n\n\tif (resp.resp.result != QMI_RESULT_SUCCESS_V01) {\n\t\ticnss_pr_err("QMI MSA Mem info request rejected, result:%d error:%d\\n",\n\t\t\tresp.resp.result, resp.resp.error);\n\t\tret = -resp.resp.result;\n\t\tgoto out;\n\t}\n\n\ticnss_pr_dbg("Receive mem_region_info_len: %d\\n",\n\t\t     resp.mem_region_info_len);\n\n\tif (resp.mem_region_info_len > QMI_WLFW_MAX_NUM_MEMORY_REGIONS_V01) {\n\t\ticnss_pr_err("Invalid memory region length received: %d\\n",\n\t\t\t     resp.mem_region_info_len);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmax_mapped_addr = penv->msa_pa + penv->msa_mem_size;\n\tpenv->stats.msa_info_resp++;\n\tpenv->nr_mem_region = resp.mem_region_info_len;\n\tfor (i = 0; i < resp.mem_region_info_len; i++) {\n\n\t\tif (resp.mem_region_info[i].size > penv->msa_mem_size ||\n\t\t    resp.mem_region_info[i].region_addr > max_mapped_addr ||\n\t\t    resp.mem_region_info[i].region_addr < penv->msa_pa ||\n\t\t    resp.mem_region_info[i].size +\n\t\t    resp.mem_region_info[i].region_addr > max_mapped_addr) {\n\t\t\ticnss_pr_dbg("Received out of range Addr: 0x%llx Size: 0x%x\\n",\n\t\t\t\t\tresp.mem_region_info[i].region_addr,\n\t\t\t\t\tresp.mem_region_info[i].size);\n\t\t\tret = -EINVAL;\n\t\t\tgoto fail_unwind;\n\t\t}\n\n\t\tpenv->mem_region[i].reg_addr =\n\t\t\tresp.mem_region_info[i].region_addr;\n\t\tpenv->mem_region[i].size =\n\t\t\tresp.mem_region_info[i].size;\n\t\tpenv->mem_region[i].secure_flag =\n\t\t\tresp.mem_region_info[i].secure_flag;\n\t\ticnss_pr_dbg("Memory Region: %d Addr: 0x%llx Size: 0x%x Flag: 0x%08x\\n",\n\t\t\t     i, penv->mem_region[i].reg_addr,\n\t\t\t     penv->mem_region[i].size,\n\t\t\t     penv->mem_region[i].secure_flag);\n\t}\n\n\treturn 0;\n\nfail_unwind:\n\tmemset(&penv->mem_region[0], 0, sizeof(penv->mem_region[0]) * i);\nout:\n\tpenv->stats.msa_info_err++;\n\tICNSS_QMI_ASSERT();\n\treturn ret;\n}'
p10
ssS'0c755962c9cc'
p11
(dp12
(g7
g8
tp13
S'static int wlfw_msa_mem_info_send_sync_msg(void)\n{\n\tint ret;\n\tint i;\n\tstruct wlfw_msa_info_req_msg_v01 req;\n\tstruct wlfw_msa_info_resp_msg_v01 resp;\n\tstruct msg_desc req_desc, resp_desc;\n\tuint64_t max_mapped_addr;\n\n\tif (!penv || !penv->wlfw_clnt)\n\t\treturn -ENODEV;\n\n\ticnss_pr_dbg("Sending MSA mem info, state: 0x%lx\\n", penv->state);\n\n\tmemset(&req, 0, sizeof(req));\n\tmemset(&resp, 0, sizeof(resp));\n\n\treq.msa_addr = penv->msa_pa;\n\treq.size = penv->msa_mem_size;\n\n\treq_desc.max_msg_len = WLFW_MSA_INFO_REQ_MSG_V01_MAX_MSG_LEN;\n\treq_desc.msg_id = QMI_WLFW_MSA_INFO_REQ_V01;\n\treq_desc.ei_array = wlfw_msa_info_req_msg_v01_ei;\n\n\tresp_desc.max_msg_len = WLFW_MSA_INFO_RESP_MSG_V01_MAX_MSG_LEN;\n\tresp_desc.msg_id = QMI_WLFW_MSA_INFO_RESP_V01;\n\tresp_desc.ei_array = wlfw_msa_info_resp_msg_v01_ei;\n\n\tpenv->stats.msa_info_req++;\n\n\tret = qmi_send_req_wait(penv->wlfw_clnt, &req_desc, &req, sizeof(req),\n\t\t\t&resp_desc, &resp, sizeof(resp), WLFW_TIMEOUT_MS);\n\tif (ret < 0) {\n\t\ticnss_pr_err("Send MSA Mem info req failed %d\\n", ret);\n\t\tgoto out;\n\t}\n\n\tif (resp.resp.result != QMI_RESULT_SUCCESS_V01) {\n\t\ticnss_pr_err("QMI MSA Mem info request rejected, result:%d error:%d\\n",\n\t\t\tresp.resp.result, resp.resp.error);\n\t\tret = -resp.resp.result;\n\t\tgoto out;\n\t}\n\n\ticnss_pr_dbg("Receive mem_region_info_len: %d\\n",\n\t\t     resp.mem_region_info_len);\n\n\tif (resp.mem_region_info_len > QMI_WLFW_MAX_NUM_MEMORY_REGIONS_V01) {\n\t\ticnss_pr_err("Invalid memory region length received: %d\\n",\n\t\t\t     resp.mem_region_info_len);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmax_mapped_addr = penv->msa_pa + penv->msa_mem_size;\n\tpenv->stats.msa_info_resp++;\n\tpenv->nr_mem_region = resp.mem_region_info_len;\n\tfor (i = 0; i < resp.mem_region_info_len; i++) {\n\n\t\tif (resp.mem_region_info[i].size > penv->msa_mem_size ||\n\t\t    resp.mem_region_info[i].region_addr >= max_mapped_addr ||\n\t\t    resp.mem_region_info[i].region_addr < penv->msa_pa ||\n\t\t    resp.mem_region_info[i].size +\n\t\t    resp.mem_region_info[i].region_addr > max_mapped_addr) {\n\t\t\ticnss_pr_dbg("Received out of range Addr: 0x%llx Size: 0x%x\\n",\n\t\t\t\t\tresp.mem_region_info[i].region_addr,\n\t\t\t\t\tresp.mem_region_info[i].size);\n\t\t\tret = -EINVAL;\n\t\t\tgoto fail_unwind;\n\t\t}\n\n\t\tpenv->mem_region[i].reg_addr =\n\t\t\tresp.mem_region_info[i].region_addr;\n\t\tpenv->mem_region[i].size =\n\t\t\tresp.mem_region_info[i].size;\n\t\tpenv->mem_region[i].secure_flag =\n\t\t\tresp.mem_region_info[i].secure_flag;\n\t\ticnss_pr_dbg("Memory Region: %d Addr: 0x%llx Size: 0x%x Flag: 0x%08x\\n",\n\t\t\t     i, penv->mem_region[i].reg_addr,\n\t\t\t     penv->mem_region[i].size,\n\t\t\t     penv->mem_region[i].secure_flag);\n\t}\n\n\treturn 0;\n\nfail_unwind:\n\tmemset(&penv->mem_region[0], 0, sizeof(penv->mem_region[0]) * i);\nout:\n\tpenv->stats.msa_info_err++;\n\tICNSS_QMI_ASSERT();\n\treturn ret;\n}'
p14
sssS'beforecommit'
p15
S'70f432282130'
p16
ssS'CVE-2019-10529'
p17
(dp18
g3
(dp19
S'a77c16d4981b'
p20
(dp21
(S'drivers/gpu/msm/kgsl.c'
p22
S'kgsl_mem_entry_destroy'
p23
tp24
S"kgsl_mem_entry_destroy(struct kref *kref)\n{\n\tstruct kgsl_mem_entry *entry = container_of(kref,\n\t\t\t\t\t\t    struct kgsl_mem_entry,\n\t\t\t\t\t\t    refcount);\n\tunsigned int memtype;\n\n\tif (entry == NULL)\n\t\treturn;\n\n\t/* pull out the memtype before the flags get cleared */\n\tmemtype = kgsl_memdesc_usermem_type(&entry->memdesc);\n\n\t/* Detach from process list */\n\tkgsl_mem_entry_detach_process(entry);\n\n\tif (memtype != KGSL_MEM_ENTRY_KERNEL)\n\t\tatomic_long_sub(entry->memdesc.size,\n\t\t\t&kgsl_driver.stats.mapped);\n\n\t/*\n\t * Ion takes care of freeing the sg_table for us so\n\t * clear the sg table before freeing the sharedmem\n\t * so kgsl_sharedmem_free doesn't try to free it again\n\t */\n\tif (memtype == KGSL_MEM_ENTRY_ION)\n\t\tentry->memdesc.sgt = NULL;\n\n\tif ((memtype == KGSL_MEM_ENTRY_USER)\n\t\t&& !(entry->memdesc.flags & KGSL_MEMFLAGS_GPUREADONLY)) {\n\t\tint i = 0, j;\n\t\tstruct scatterlist *sg;\n\t\tstruct page *page;\n\t\t/*\n\t\t * Mark all of pages in the scatterlist as dirty since they\n\t\t * were writable by the GPU.\n\t\t */\n\t\tfor_each_sg(entry->memdesc.sgt->sgl, sg,\n\t\t\t    entry->memdesc.sgt->nents, i) {\n\t\t\tpage = sg_page(sg);\n\t\t\tfor (j = 0; j < (sg->length >> PAGE_SHIFT); j++)\n\t\t\t\tset_page_dirty_lock(nth_page(page, j));\n\t\t}\n\t}\n\n\tkgsl_sharedmem_free(&entry->memdesc);\n\n\tswitch (memtype) {\n\tcase KGSL_MEM_ENTRY_ION:\n\t\tkgsl_destroy_ion(entry->priv_data);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tkfree(entry);\n}"
p25
sssg15
S'70f432282130'
p26
ssS'CVE-2019-10515'
p27
(dp28
g3
(dp29
S'f332617ebb03'
p30
(dp31
(S'drivers/char/diag/diag_dci.c'
p32
S'dci_lookup_client_entry_pid'
p33
tp34
S'struct diag_dci_client_tbl *dci_lookup_client_entry_pid(int tgid)\n{\n\tstruct list_head *start, *temp;\n\tstruct diag_dci_client_tbl *entry = NULL;\n\tstruct pid *pid_struct = NULL;\n\tstruct task_struct *task_s = NULL;\n\n\tlist_for_each_safe(start, temp, &driver->dci_client_list) {\n\t\tentry = list_entry(start, struct diag_dci_client_tbl, track);\n\t\tpid_struct = find_get_pid(entry->tgid);\n\t\tif (!pid_struct) {\n\t\t\tDIAG_LOG(DIAG_DEBUG_DCI,\n\t\t\t"diag: Exited pid (%d) doesn\'t match dci client of pid (%d)\\n",\n\t\t\ttgid, entry->tgid);\n\t\t\tcontinue;\n\t\t}\n\t\ttask_s = get_pid_task(pid_struct, PIDTYPE_PID);\n\t\tif (!task_s) {\n\t\t\tDIAG_LOG(DIAG_DEBUG_DCI,\n\t\t\t\t"diag: valid task doesn\'t exist for pid = %d\\n",\n\t\t\t\tentry->tgid);\n\t\t\tcontinue;\n\t\t}\n\t\tif (task_s == entry->client)\n\t\t\tif (entry->client->tgid == tgid)\n\t\t\t\treturn entry;\n\t}\n\treturn NULL;\n}'
p35
ssS'6c6aaf4e8330'
p36
(dp37
(g32
g33
tp38
S'struct diag_dci_client_tbl *dci_lookup_client_entry_pid(int tgid)\n{\n\tstruct list_head *start, *temp;\n\tstruct diag_dci_client_tbl *entry = NULL;\n\tstruct pid *pid_struct = NULL;\n\tstruct task_struct *task_s = NULL;\n\n\tlist_for_each_safe(start, temp, &driver->dci_client_list) {\n\t\tentry = list_entry(start, struct diag_dci_client_tbl, track);\n\t\tpid_struct = find_get_pid(entry->tgid);\n\t\tif (!pid_struct) {\n\t\t\tDIAG_LOG(DIAG_DEBUG_DCI,\n\t\t\t"diag: Exited pid (%d) doesn\'t match dci client of pid (%d)\\n",\n\t\t\ttgid, entry->tgid);\n\t\t\tcontinue;\n\t\t}\n\t\ttask_s = get_pid_task(pid_struct, PIDTYPE_PID);\n\t\tif (!task_s) {\n\t\t\tDIAG_LOG(DIAG_DEBUG_DCI,\n\t\t\t\t"diag: valid task doesn\'t exist for pid = %d\\n",\n\t\t\t\tentry->tgid);\n\t\t\tput_pid(pid_struct);\n\t\t\tcontinue;\n\t\t}\n\t\tif (task_s == entry->client) {\n\t\t\tif (entry->client->tgid == tgid) {\n\t\t\t\tput_task_struct(task_s);\n\t\t\t\tput_pid(pid_struct);\n\t\t\t\treturn entry;\n\t\t\t}\n\t\t}\n\t\tput_task_struct(task_s);\n\t\tput_pid(pid_struct);\n\t}\n\treturn NULL;\n}'
p39
sssg15
S'ad6d811a2827'
p40
ssS'CVE-2019-2279'
p41
(dp42
g3
(dp43
S'774fe2ef649e'
p44
(dp45
(S'drivers/media/platform/msm/vidc/venus_hfi.c'
p46
S'__write_queue'
p47
tp48
S'static int __write_queue(struct vidc_iface_q_info *qinfo, u8 *packet,\n\t\tbool *rx_req_is_set)\n{\n\tstruct hfi_queue_header *queue;\n\tu32 packet_size_in_words, new_write_idx;\n\tu32 empty_space, read_idx, write_idx;\n\tu32 *write_ptr;\n\n\tif (!qinfo || !packet) {\n\t\tdprintk(VIDC_ERR, "Invalid Params\\n");\n\t\treturn -EINVAL;\n\t} else if (!qinfo->q_array.align_virtual_addr) {\n\t\tdprintk(VIDC_WARN, "Queues have already been freed\\n");\n\t\treturn -EINVAL;\n\t}\n\n\tqueue = (struct hfi_queue_header *) qinfo->q_hdr;\n\tif (!queue) {\n\t\tdprintk(VIDC_ERR, "queue not present\\n");\n\t\treturn -ENOENT;\n\t}\n\n\tif (msm_vidc_debug & VIDC_PKT) {\n\t\tdprintk(VIDC_PKT, "%s: %pK\\n", __func__, qinfo);\n\t\t__dump_packet(packet, VIDC_PKT);\n\t}\n\n\tpacket_size_in_words = (*(u32 *)packet) >> 2;\n\tif (!packet_size_in_words || packet_size_in_words >\n\t\tqinfo->q_array.mem_size>>2) {\n\t\tdprintk(VIDC_ERR, "Invalid packet size\\n");\n\t\treturn -ENODATA;\n\t}\n\n\tread_idx = queue->qhdr_read_idx;\n\twrite_idx = queue->qhdr_write_idx;\n\n\tempty_space = (write_idx >=  read_idx) ?\n\t\t((qinfo->q_array.mem_size>>2) - (write_idx -  read_idx)) :\n\t\t(read_idx - write_idx);\n\tif (empty_space <= packet_size_in_words) {\n\t\tqueue->qhdr_tx_req =  1;\n\t\tdprintk(VIDC_ERR, "Insufficient size (%d) to write (%d)\\n",\n\t\t\t\t\t  empty_space, packet_size_in_words);\n\t\treturn -ENOTEMPTY;\n\t}\n\n\tqueue->qhdr_tx_req =  0;\n\n\tnew_write_idx = write_idx + packet_size_in_words;\n\twrite_ptr = (u32 *)((qinfo->q_array.align_virtual_addr) +\n\t\t\t(write_idx << 2));\n\tif (write_ptr < (u32 *)qinfo->q_array.align_virtual_addr ||\n\t    write_ptr > (u32 *)(qinfo->q_array.align_virtual_addr +\n\t    qinfo->q_array.mem_size)) {\n\t\tdprintk(VIDC_ERR, "Invalid write index");\n\t\treturn -ENODATA;\n\t}\n\n\tif (new_write_idx < (qinfo->q_array.mem_size >> 2)) {\n\t\tmemcpy(write_ptr, packet, packet_size_in_words << 2);\n\t} else {\n\t\tnew_write_idx -= qinfo->q_array.mem_size >> 2;\n\t\tmemcpy(write_ptr, packet, (packet_size_in_words -\n\t\t\tnew_write_idx) << 2);\n\t\tmemcpy((void *)qinfo->q_array.align_virtual_addr,\n\t\t\tpacket + ((packet_size_in_words - new_write_idx) << 2),\n\t\t\tnew_write_idx  << 2);\n\t}\n\n\t/*\n\t * Memory barrier to make sure packet is written before updating the\n\t * write index\n\t */\n\tmb();\n\tqueue->qhdr_write_idx = new_write_idx;\n\tif (rx_req_is_set)\n\t\t*rx_req_is_set = queue->qhdr_rx_req == 1;\n\t/*\n\t * Memory barrier to make sure write index is updated before an\n\t * interrupt is raised on venus.\n\t */\n\tmb();\n\treturn 0;\n}'
p49
s(g46
S'__read_queue'
p50
tp51
S'static int __read_queue(struct vidc_iface_q_info *qinfo, u8 *packet,\n\t\tu32 *pb_tx_req_is_set)\n{\n\tstruct hfi_queue_header *queue;\n\tu32 packet_size_in_words, new_read_idx;\n\tu32 *read_ptr;\n\tu32 receive_request = 0;\n\tu32 read_idx, write_idx;\n\tint rc = 0;\n\n\tif (!qinfo || !packet || !pb_tx_req_is_set) {\n\t\tdprintk(VIDC_ERR, "Invalid Params\\n");\n\t\treturn -EINVAL;\n\t} else if (!qinfo->q_array.align_virtual_addr) {\n\t\tdprintk(VIDC_WARN, "Queues have already been freed\\n");\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Memory barrier to make sure data is valid before\n\t *reading it\n\t */\n\tmb();\n\tqueue = (struct hfi_queue_header *) qinfo->q_hdr;\n\n\tif (!queue) {\n\t\tdprintk(VIDC_ERR, "Queue memory is not allocated\\n");\n\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * Do not set receive request for debug queue, if set,\n\t * Venus generates interrupt for debug messages even\n\t * when there is no response message available.\n\t * In general debug queue will not become full as it\n\t * is being emptied out for every interrupt from Venus.\n\t * Venus will anyway generates interrupt if it is full.\n\t */\n\tif (queue->qhdr_type & HFI_Q_ID_CTRL_TO_HOST_MSG_Q)\n\t\treceive_request = 1;\n\n\tread_idx = queue->qhdr_read_idx;\n\twrite_idx = queue->qhdr_write_idx;\n\n\tif (read_idx == write_idx) {\n\t\tqueue->qhdr_rx_req = receive_request;\n\t\t/*\n\t\t * mb() to ensure qhdr is updated in main memory\n\t\t * so that venus reads the updated header values\n\t\t */\n\t\tmb();\n\t\t*pb_tx_req_is_set = 0;\n\t\tdprintk(VIDC_DBG,\n\t\t\t"%s queue is empty, rx_req = %u, tx_req = %u, read_idx = %u\\n",\n\t\t\treceive_request ? "message" : "debug",\n\t\t\tqueue->qhdr_rx_req, queue->qhdr_tx_req,\n\t\t\tqueue->qhdr_read_idx);\n\t\treturn -ENODATA;\n\t}\n\n\tread_ptr = (u32 *)((qinfo->q_array.align_virtual_addr) +\n\t\t\t\t(read_idx << 2));\n\tif (read_ptr < (u32 *)qinfo->q_array.align_virtual_addr ||\n\t    read_ptr > (u32 *)(qinfo->q_array.align_virtual_addr +\n\t    qinfo->q_array.mem_size - sizeof(*read_ptr))) {\n\t\tdprintk(VIDC_ERR, "Invalid read index\\n");\n\t\treturn -ENODATA;\n\t}\n\n\tpacket_size_in_words = (*read_ptr) >> 2;\n\tif (!packet_size_in_words) {\n\t\tdprintk(VIDC_ERR, "Zero packet size\\n");\n\t\treturn -ENODATA;\n\t}\n\n\tnew_read_idx = read_idx + packet_size_in_words;\n\tif (((packet_size_in_words << 2) <= VIDC_IFACEQ_VAR_HUGE_PKT_SIZE) &&\n\t\tread_idx <= (qinfo->q_array.mem_size >> 2)) {\n\t\tif (new_read_idx < (qinfo->q_array.mem_size >> 2)) {\n\t\t\tmemcpy(packet, read_ptr,\n\t\t\t\t\tpacket_size_in_words << 2);\n\t\t} else {\n\t\t\tnew_read_idx -= (qinfo->q_array.mem_size >> 2);\n\t\t\tmemcpy(packet, read_ptr,\n\t\t\t(packet_size_in_words - new_read_idx) << 2);\n\t\t\tmemcpy(packet + ((packet_size_in_words -\n\t\t\t\t\tnew_read_idx) << 2),\n\t\t\t\t\t(u8 *)qinfo->q_array.align_virtual_addr,\n\t\t\t\t\tnew_read_idx << 2);\n\t\t}\n\t} else {\n\t\tdprintk(VIDC_WARN,\n\t\t\t"BAD packet received, read_idx: %#x, pkt_size: %d\\n",\n\t\t\tread_idx, packet_size_in_words << 2);\n\t\tdprintk(VIDC_WARN, "Dropping this packet\\n");\n\t\tnew_read_idx = write_idx;\n\t\trc = -ENODATA;\n\t}\n\n\tif (new_read_idx != write_idx)\n\t\tqueue->qhdr_rx_req = 0;\n\telse\n\t\tqueue->qhdr_rx_req = receive_request;\n\n\tqueue->qhdr_read_idx = new_read_idx;\n\t/*\n\t * mb() to ensure qhdr is updated in main memory\n\t * so that venus reads the updated header values\n\t */\n\tmb();\n\n\t*pb_tx_req_is_set = (queue->qhdr_tx_req == 1) ? 1 : 0;\n\n\tif ((msm_vidc_debug & VIDC_PKT) &&\n\t\t!(queue->qhdr_type & HFI_Q_ID_CTRL_TO_HOST_DEBUG_Q)) {\n\t\tdprintk(VIDC_PKT, "%s: %pK\\n", __func__, qinfo);\n\t\t__dump_packet(packet, VIDC_PKT);\n\t}\n\n\treturn rc;\n}'
p52
sssg15
S'1d7b97667c43'
p53
ssS'CVE-2018-19824'
p54
(dp55
g3
(dp56
S'c36d54c34fef'
p57
(dp58
(S'sound/usb/card.c'
p59
S'usb_audio_probe'
p60
tp61
S'static int usb_audio_probe(struct usb_interface *intf,\n\t\t\t   const struct usb_device_id *usb_id)\n{\n\tstruct usb_device *dev = interface_to_usbdev(intf);\n\tconst struct snd_usb_audio_quirk *quirk =\n\t\t(const struct snd_usb_audio_quirk *)usb_id->driver_info;\n\tstruct snd_usb_audio *chip;\n\tint i, err;\n\tstruct usb_host_interface *alts;\n\tint ifnum;\n\tu32 id;\n\tstruct usb_interface_assoc_descriptor *assoc;\n\n\tassoc = intf->intf_assoc;\n\tif (assoc && assoc->bFunctionClass == USB_CLASS_AUDIO &&\n\t    assoc->bFunctionProtocol == UAC_VERSION_3 &&\n\t    assoc->bFunctionSubClass == FULL_ADC_3_0) {\n\t\tdev_info(&dev->dev, "No support for full-fledged ADC 3.0 yet!!\\n");\n\t\treturn -EINVAL;\n\t}\n\n\talts = &intf->altsetting[0];\n\tifnum = get_iface_desc(alts)->bInterfaceNumber;\n\tid = USB_ID(le16_to_cpu(dev->descriptor.idVendor),\n\t\t    le16_to_cpu(dev->descriptor.idProduct));\n\tif (get_alias_id(dev, &id))\n\t\tquirk = get_alias_quirk(dev, id);\n\tif (quirk && quirk->ifnum >= 0 && ifnum != quirk->ifnum)\n\t\treturn -ENXIO;\n\n\terr = snd_usb_apply_boot_quirk(dev, intf, quirk, id);\n\tif (err < 0)\n\t\treturn err;\n\n\t/*\n\t * found a config.  now register to ALSA\n\t */\n\n\t/* check whether it\'s already registered */\n\tchip = NULL;\n\tmutex_lock(&register_mutex);\n\tfor (i = 0; i < SNDRV_CARDS; i++) {\n\t\tif (usb_chip[i] && usb_chip[i]->dev == dev) {\n\t\t\tif (atomic_read(&usb_chip[i]->shutdown)) {\n\t\t\t\tdev_err(&dev->dev, "USB device is in the shutdown state, cannot create a card instance\\n");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto __error;\n\t\t\t}\n\t\t\tchip = usb_chip[i];\n\t\t\tatomic_inc(&chip->active); /* avoid autopm */\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (! chip) {\n\t\t/* it\'s a fresh one.\n\t\t * now look for an empty slot and create a new card instance\n\t\t */\n\t\tfor (i = 0; i < SNDRV_CARDS; i++)\n\t\t\tif (enable[i] && ! usb_chip[i] &&\n\t\t\t    (vid[i] == -1 || vid[i] == USB_ID_VENDOR(id)) &&\n\t\t\t    (pid[i] == -1 || pid[i] == USB_ID_PRODUCT(id))) {\n\t\t\t\terr = snd_usb_audio_create(intf, dev, i, quirk,\n\t\t\t\t\t\t\t   &chip);\n\t\t\t\tif (err < 0)\n\t\t\t\t\tgoto __error;\n\t\t\t\tchip->pm_intf = intf;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tif (!chip) {\n\t\t\tdev_err(&dev->dev, "no available usb audio device\\n");\n\t\t\terr = -ENODEV;\n\t\t\tgoto __error;\n\t\t}\n\t}\n\tdev_set_drvdata(&dev->dev, chip);\n\n\t/*\n\t * For devices with more than one control interface, we assume the\n\t * first contains the audio controls. We might need a more specific\n\t * check here in the future.\n\t */\n\tif (!chip->ctrl_intf)\n\t\tchip->ctrl_intf = alts;\n\n\tchip->txfr_quirk = 0;\n\terr = 1; /* continue */\n\tif (quirk && quirk->ifnum != QUIRK_NO_INTERFACE) {\n\t\t/* need some special handlings */\n\t\terr = snd_usb_create_quirk(chip, intf, &usb_audio_driver, quirk);\n\t\tif (err < 0)\n\t\t\tgoto __error;\n\t}\n\n\tif (err > 0) {\n\t\t/* create normal USB audio interfaces */\n\t\terr = snd_usb_create_streams(chip, ifnum);\n\t\tif (err < 0)\n\t\t\tgoto __error;\n\t\terr = snd_usb_create_mixer(chip, ifnum, ignore_ctl_error);\n\t\tif (err < 0)\n\t\t\tgoto __error;\n\t}\n\n\t/* we are allowed to call snd_card_register() many times */\n\terr = snd_card_register(chip->card);\n\tif (err < 0)\n\t\tgoto __error;\n\n\tusb_chip[chip->index] = chip;\n\tchip->num_interfaces++;\n\tusb_set_intfdata(intf, chip);\n\tintf->needs_remote_wakeup = 1;\n\tusb_enable_autosuspend(chip->dev);\n\tatomic_dec(&chip->active);\n\tmutex_unlock(&register_mutex);\n\treturn 0;\n\n __error:\n\tif (chip) {\n\t\t/* chip->active is inside the chip->card object,\n\t\t * decrement before memory is possibly returned.\n\t\t */\n\t\tatomic_dec(&chip->active);\n\t\tif (!chip->num_interfaces)\n\t\t\tsnd_card_free(chip->card);\n\t}\n\tmutex_unlock(&register_mutex);\n\treturn err;\n}'
p62
sssg15
S'a284d4c9bbea'
p63
ssS'CVE-2019-10524'
p64
(dp65
g3
(dp66
S'9393739f2aa4'
p67
(dp68
(S'drivers/media/platform/msm/camera_v2/common/msm_camera_io_util.c'
p69
S'msm_cam_clk_enable'
p70
tp71
S'int msm_cam_clk_enable(struct device *dev, struct msm_cam_clk_info *clk_info,\n\t\tstruct clk **clk_ptr, int num_clk, int enable)\n{\n\tint i;\n\tint rc = 0;\n\tlong clk_rate;\n\n\tif (enable) {\n\t\tfor (i = 0; i < num_clk; i++) {\n\t\t\tCDBG("%s enable %s\\n", __func__, clk_info[i].clk_name);\n\t\t\tclk_ptr[i] = clk_get(dev, clk_info[i].clk_name);\n\t\t\tif (IS_ERR(clk_ptr[i])) {\n\t\t\t\tpr_err("%s get failed\\n", clk_info[i].clk_name);\n\t\t\t\trc = PTR_ERR(clk_ptr[i]);\n\t\t\t\tgoto cam_clk_get_err;\n\t\t\t}\n\t\t\tif (clk_info[i].clk_rate > 0) {\n\t\t\t\tclk_rate = clk_round_rate(clk_ptr[i],\n\t\t\t\t\tclk_info[i].clk_rate);\n\t\t\t\tif (clk_rate < 0) {\n\t\t\t\t\tpr_err("%s round failed\\n",\n\t\t\t\t\t\t   clk_info[i].clk_name);\n\t\t\t\t\tgoto cam_clk_set_err;\n\t\t\t\t}\n\t\t\t\trc = clk_set_rate(clk_ptr[i],\n\t\t\t\t\tclk_rate);\n\t\t\t\tif (rc < 0) {\n\t\t\t\t\tpr_err("%s set failed\\n",\n\t\t\t\t\t\tclk_info[i].clk_name);\n\t\t\t\t\tgoto cam_clk_set_err;\n\t\t\t\t}\n\n\t\t\t} else if (clk_info[i].clk_rate == INIT_RATE) {\n\t\t\t\tclk_rate = clk_get_rate(clk_ptr[i]);\n\t\t\t\tif (clk_rate == 0) {\n\t\t\t\t\tclk_rate =\n\t\t\t\t\t\t  clk_round_rate(clk_ptr[i], 0);\n\t\t\t\t\tif (clk_rate < 0) {\n\t\t\t\t\t\tpr_err("%s round rate failed\\n",\n\t\t\t\t\t\t\t  clk_info[i].clk_name);\n\t\t\t\t\t\tgoto cam_clk_set_err;\n\t\t\t\t\t}\n\t\t\t\t\trc = clk_set_rate(clk_ptr[i],\n\t\t\t\t\t\t\t\tclk_rate);\n\t\t\t\t\tif (rc < 0) {\n\t\t\t\t\t\tpr_err("%s set rate failed\\n",\n\t\t\t\t\t\t\t  clk_info[i].clk_name);\n\t\t\t\t\t\tgoto cam_clk_set_err;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\trc = clk_prepare(clk_ptr[i]);\n\t\t\tif (rc < 0) {\n\t\t\t\tpr_err("%s prepare failed\\n",\n\t\t\t\t\t   clk_info[i].clk_name);\n\t\t\t\tgoto cam_clk_prepare_err;\n\t\t\t}\n\n\t\t\trc = clk_enable(clk_ptr[i]);\n\t\t\tif (rc < 0) {\n\t\t\t\tpr_err("%s enable failed\\n",\n\t\t\t\t\t   clk_info[i].clk_name);\n\t\t\t\tgoto cam_clk_enable_err;\n\t\t\t}\n\t\t\tif (clk_info[i].delay > 20) {\n\t\t\t\tmsleep(clk_info[i].delay);\n\t\t\t} else if (clk_info[i].delay) {\n\t\t\t\tusleep_range(clk_info[i].delay * 1000,\n\t\t\t\t\t(clk_info[i].delay * 1000) + 1000);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor (i = num_clk - 1; i >= 0; i--) {\n\t\t\tif (!IS_ERR_OR_NULL(clk_ptr[i])) {\n\t\t\t\tCDBG("%s disable %s\\n", __func__,\n\t\t\t\t\tclk_info[i].clk_name);\n\t\t\t\tclk_disable(clk_ptr[i]);\n\t\t\t\tclk_unprepare(clk_ptr[i]);\n\t\t\t\tclk_put(clk_ptr[i]);\n\t\t\t\tclk_ptr[i] = NULL;\n\t\t\t}\n\t\t}\n\t}\n\treturn rc;\n\n\ncam_clk_enable_err:\n\tclk_unprepare(clk_ptr[i]);\ncam_clk_prepare_err:\ncam_clk_set_err:\n\tclk_put(clk_ptr[i]);\ncam_clk_get_err:\n\tfor (i--; i >= 0; i--) {\n\t\tif (!IS_ERR_OR_NULL(clk_ptr[i])) {\n\t\t\tclk_disable(clk_ptr[i]);\n\t\t\tclk_unprepare(clk_ptr[i]);\n\t\t\tclk_put(clk_ptr[i]);\n\t\t\tclk_ptr[i] = NULL;\n\t\t}\n\t}\n\treturn rc;\n}'
p72
sssg15
S'774fe2ef649e'
p73
ssS'CVE-2019-2308'
p74
(dp75
g3
(dp76
S'1d1a62b98700'
p77
(dp78
(S'drivers/char/adsprpc.c'
p79
S'fastrpc_internal_invoke'
p80
tp81
S'static int fastrpc_internal_invoke(struct fastrpc_file *fl, uint32_t mode,\n\t\t\t\t   uint32_t kernel,\n\t\t\t\t   struct fastrpc_ioctl_invoke_crc *inv)\n{\n\tstruct smq_invoke_ctx *ctx = NULL;\n\tstruct fastrpc_ioctl_invoke *invoke = &inv->inv;\n\tint cid = fl->cid;\n\tint interrupted = 0;\n\tint err = 0;\n\tstruct timespec invoket = {0};\n\tint64_t *perf_counter = getperfcounter(fl, PERF_COUNT);\n\n\tif (fl->profile)\n\t\tgetnstimeofday(&invoket);\n\n\tif (!kernel) {\n\t\tVERIFY(err, invoke->handle != FASTRPC_STATIC_HANDLE_KERNEL);\n\t\tif (err) {\n\t\t\tpr_err("adsprpc: ERROR: %s: user application %s trying to send a kernel RPC message to channel %d",\n\t\t\t\t__func__, current->comm, cid);\n\t\t\tgoto bail;\n\t\t}\n\t}\n\n\tVERIFY(err, fl->sctx != NULL);\n\tif (err)\n\t\tgoto bail;\n\tVERIFY(err, fl->cid >= 0 && fl->cid < NUM_CHANNELS);\n\tif (err)\n\t\tgoto bail;\n\n\tif (!kernel) {\n\t\tVERIFY(err, 0 == context_restore_interrupted(fl, inv,\n\t\t\t\t\t\t\t\t&ctx));\n\t\tif (err)\n\t\t\tgoto bail;\n\t\tif (fl->sctx->smmu.faults)\n\t\t\terr = FASTRPC_ENOSUCH;\n\t\tif (err)\n\t\t\tgoto bail;\n\t\tif (ctx)\n\t\t\tgoto wait;\n\t}\n\n\tVERIFY(err, 0 == context_alloc(fl, kernel, inv, &ctx));\n\tif (err)\n\t\tgoto bail;\n\n\tif (REMOTE_SCALARS_LENGTH(ctx->sc)) {\n\t\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_GETARGS),\n\t\tVERIFY(err, 0 == get_args(kernel, ctx));\n\t\tPERF_END);\n\t\tif (err)\n\t\t\tgoto bail;\n\t}\n\n\tif (!fl->sctx->smmu.coherent) {\n\t\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_INVARGS),\n\t\tinv_args_pre(ctx);\n\t\tPERF_END);\n\t}\n\n\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_LINK),\n\tVERIFY(err, 0 == fastrpc_invoke_send(ctx, kernel, invoke->handle));\n\tPERF_END);\n\n\tif (err)\n\t\tgoto bail;\n wait:\n\tif (kernel)\n\t\twait_for_completion(&ctx->work);\n\telse {\n\t\tinterrupted = wait_for_completion_interruptible(&ctx->work);\n\t\tVERIFY(err, 0 == (err = interrupted));\n\t\tif (err)\n\t\t\tgoto bail;\n\t}\n\tif (ctx->handle)\n\t\tglink_rx_done(ctx->handle, ctx->ptr, true);\n\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_INVARGS),\n\tif (!fl->sctx->smmu.coherent)\n\t\tinv_args(ctx);\n\tPERF_END);\n\n\tVERIFY(err, 0 == (err = ctx->retval));\n\tif (err)\n\t\tgoto bail;\n\n\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_PUTARGS),\n\tVERIFY(err, 0 == put_args(kernel, ctx, invoke->pra));\n\tPERF_END);\n\tif (err)\n\t\tgoto bail;\n bail:\n\tif (ctx && interrupted == -ERESTARTSYS)\n\t\tcontext_save_interrupted(ctx);\n\telse if (ctx)\n\t\tcontext_free(ctx);\n\tif (fl->ssrcount != fl->apps->channel[cid].ssrcount)\n\t\terr = ECONNRESET;\n\n\tif (fl->profile && !interrupted) {\n\t\tif (invoke->handle != FASTRPC_STATIC_HANDLE_LISTENER) {\n\t\t\tint64_t *count = GET_COUNTER(perf_counter, PERF_INVOKE);\n\n\t\t\tif (count)\n\t\t\t\t*count += getnstimediff(&invoket);\n\t\t}\n\t\tif (invoke->handle > FASTRPC_STATIC_HANDLE_MAX) {\n\t\t\tint64_t *count = GET_COUNTER(perf_counter, PERF_COUNT);\n\n\t\t\tif (count)\n\t\t\t\t*count = *count+1;\n\t\t}\n\t}\n\treturn err;\n}'
p82
ssS'058ddcee4c12'
p83
(dp84
(g79
S'fastrpc_munmap_on_dsp_rh'
p85
tp86
S'static int fastrpc_munmap_on_dsp_rh(struct fastrpc_file *fl, uint64_t phys,\n\t\t\t\t\t\tsize_t size, uint32_t flags)\n{\n\tint err = 0;\n\tstruct fastrpc_apps *me = &gfa;\n\tint destVM[1] = {VMID_HLOS};\n\tint destVMperm[1] = {PERM_READ | PERM_WRITE | PERM_EXEC};\n\n\tif (flags == ADSP_MMAP_HEAP_ADDR) {\n\t\tstruct fastrpc_ioctl_invoke_crc ioctl;\n\t\tstruct scm_desc desc = {0};\n\t\tremote_arg_t ra[1];\n\t\tint err = 0;\n\t\tstruct {\n\t\t\tuint8_t skey;\n\t\t} routargs;\n\n\t\tra[0].buf.pv = (void *)&routargs;\n\t\tra[0].buf.len = sizeof(routargs);\n\n\t\tioctl.inv.handle = FASTRPC_STATIC_HANDLE_KERNEL;\n\t\tioctl.inv.sc = REMOTE_SCALARS_MAKE(7, 0, 1);\n\t\tioctl.inv.pra = ra;\n\t\tioctl.fds = NULL;\n\t\tioctl.attrs = NULL;\n\t\tioctl.crc = NULL;\n\t\tif (fl == NULL)\n\t\t\tgoto bail;\n\n\t\tVERIFY(err, 0 == (err = fastrpc_internal_invoke(fl,\n\t\t\t\tFASTRPC_MODE_PARALLEL, 1, &ioctl)));\n\t\tif (err)\n\t\t\tgoto bail;\n\t\tdesc.args[0] = TZ_PIL_AUTH_QDSP6_PROC;\n\t\tdesc.args[1] = phys;\n\t\tdesc.args[2] = size;\n\t\tdesc.args[3] = routargs.skey;\n\t\tdesc.arginfo = SCM_ARGS(4);\n\t\terr = scm_call2(SCM_SIP_FNID(SCM_SVC_PIL,\n\t\t\tTZ_PIL_CLEAR_PROTECT_MEM_SUBSYS_ID), &desc);\n\t} else if (flags == ADSP_MMAP_REMOTE_HEAP_ADDR) {\n\t\tVERIFY(err, !hyp_assign_phys(phys, (uint64_t)size,\n\t\t\t\t\tme->channel[fl->cid].rhvm.vmid,\n\t\t\t\t\tme->channel[fl->cid].rhvm.vmcount,\n\t\t\t\t\tdestVM, destVMperm, 1));\n\t\tif (err)\n\t\t\tgoto bail;\n\t}\n\nbail:\n\treturn err;\n}'
p87
s(g79
S'fastrpc_mmap_on_dsp'
p88
tp89
S'static int fastrpc_mmap_on_dsp(struct fastrpc_file *fl, uint32_t flags,\n\t\t\t\t\tuintptr_t va, uint64_t phys,\n\t\t\t\t\tsize_t size, uintptr_t *raddr)\n{\n\tstruct fastrpc_ioctl_invoke_crc ioctl;\n\tstruct fastrpc_apps *me = &gfa;\n\tstruct smq_phy_page page;\n\tint num = 1;\n\tremote_arg_t ra[3];\n\tint err = 0;\n\tstruct {\n\t\tint pid;\n\t\tuint32_t flags;\n\t\tuintptr_t vaddrin;\n\t\tint num;\n\t} inargs;\n\tstruct {\n\t\tuintptr_t vaddrout;\n\t} routargs;\n\n\tinargs.pid = fl->tgid;\n\tinargs.vaddrin = (uintptr_t)va;\n\tinargs.flags = flags;\n\tinargs.num = fl->apps->compat ? num * sizeof(page) : num;\n\tra[0].buf.pv = (void *)&inargs;\n\tra[0].buf.len = sizeof(inargs);\n\tpage.addr = phys;\n\tpage.size = size;\n\tra[1].buf.pv = (void *)&page;\n\tra[1].buf.len = num * sizeof(page);\n\n\tra[2].buf.pv = (void *)&routargs;\n\tra[2].buf.len = sizeof(routargs);\n\n\tioctl.inv.handle = FASTRPC_STATIC_HANDLE_KERNEL;\n\tif (fl->apps->compat)\n\t\tioctl.inv.sc = REMOTE_SCALARS_MAKE(4, 2, 1);\n\telse\n\t\tioctl.inv.sc = REMOTE_SCALARS_MAKE(2, 2, 1);\n\tioctl.inv.pra = ra;\n\tioctl.fds = NULL;\n\tioctl.attrs = NULL;\n\tioctl.crc = NULL;\n\tVERIFY(err, 0 == (err = fastrpc_internal_invoke(fl,\n\t\tFASTRPC_MODE_PARALLEL, 1, &ioctl)));\n\t*raddr = (uintptr_t)routargs.vaddrout;\n\tif (err)\n\t\tgoto bail;\n\tif (flags == ADSP_MMAP_HEAP_ADDR) {\n\t\tstruct scm_desc desc = {0};\n\n\t\tdesc.args[0] = TZ_PIL_AUTH_QDSP6_PROC;\n\t\tdesc.args[1] = phys;\n\t\tdesc.args[2] = size;\n\t\tdesc.arginfo = SCM_ARGS(3);\n\t\terr = scm_call2(SCM_SIP_FNID(SCM_SVC_PIL,\n\t\t\tTZ_PIL_PROTECT_MEM_SUBSYS_ID), &desc);\n\t} else if (flags == ADSP_MMAP_REMOTE_HEAP_ADDR) {\n\t\tVERIFY(err, !hyp_assign_phys(phys, (uint64_t)size,\n\t\t\t\thlosvm, 1, me->channel[fl->cid].rhvm.vmid,\n\t\t\t\tme->channel[fl->cid].rhvm.vmperm,\n\t\t\t\tme->channel[fl->cid].rhvm.vmcount));\n\t\tif (err)\n\t\t\tgoto bail;\n\t}\nbail:\n\treturn err;\n}'
p90
s(g79
g80
tp91
S'static int fastrpc_internal_invoke(struct fastrpc_file *fl, uint32_t mode,\n\t\t\t\t   uint32_t kernel,\n\t\t\t\t   struct fastrpc_ioctl_invoke_crc *inv)\n{\n\tstruct smq_invoke_ctx *ctx = NULL;\n\tstruct fastrpc_ioctl_invoke *invoke = &inv->inv;\n\tint cid = fl->cid;\n\tint interrupted = 0;\n\tint err = 0;\n\tstruct timespec invoket = {0};\n\tint64_t *perf_counter = getperfcounter(fl, PERF_COUNT);\n\n\tif (fl->profile)\n\t\tgetnstimeofday(&invoket);\n\n\tif (!kernel) {\n\t\tVERIFY(err, invoke->handle != FASTRPC_STATIC_HANDLE_KERNEL);\n\t\tif (err) {\n\t\t\tpr_err("adsprpc: ERROR: %s: user application %s trying to send a kernel RPC message to channel %d",\n\t\t\t\t__func__, current->comm, cid);\n\t\t\tgoto bail;\n\t\t}\n\t}\n\n\tVERIFY(err, fl->sctx != NULL);\n\tif (err)\n\t\tgoto bail;\n\tVERIFY(err, fl->cid >= 0 && fl->cid < NUM_CHANNELS);\n\tif (err)\n\t\tgoto bail;\n\n\tif (!kernel) {\n\t\tVERIFY(err, 0 == context_restore_interrupted(fl, inv,\n\t\t\t\t\t\t\t\t&ctx));\n\t\tif (err)\n\t\t\tgoto bail;\n\t\tif (fl->sctx->smmu.faults)\n\t\t\terr = FASTRPC_ENOSUCH;\n\t\tif (err)\n\t\t\tgoto bail;\n\t\tif (ctx)\n\t\t\tgoto wait;\n\t}\n\n\tVERIFY(err, 0 == context_alloc(fl, kernel, inv, &ctx));\n\tif (err)\n\t\tgoto bail;\n\n\tif (REMOTE_SCALARS_LENGTH(ctx->sc)) {\n\t\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_GETARGS),\n\t\tVERIFY(err, 0 == get_args(kernel, ctx));\n\t\tPERF_END);\n\t\tif (err)\n\t\t\tgoto bail;\n\t}\n\n\tif (!fl->sctx->smmu.coherent) {\n\t\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_INVARGS),\n\t\tinv_args_pre(ctx);\n\t\tPERF_END);\n\t}\n\n\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_LINK),\n\tVERIFY(err, 0 == fastrpc_invoke_send(ctx, kernel, invoke->handle));\n\tPERF_END);\n\n\tif (err)\n\t\tgoto bail;\n wait:\n\tif (kernel)\n\t\twait_for_completion(&ctx->work);\n\telse {\n\t\tinterrupted = wait_for_completion_interruptible(&ctx->work);\n\t\tVERIFY(err, 0 == (err = interrupted));\n\t\tif (err)\n\t\t\tgoto bail;\n\t}\n\n\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_INVARGS),\n\tif (!fl->sctx->smmu.coherent)\n\t\tinv_args(ctx);\n\tPERF_END);\n\n\tVERIFY(err, 0 == (err = ctx->retval));\n\tif (err)\n\t\tgoto bail;\n\n\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_PUTARGS),\n\tVERIFY(err, 0 == put_args(kernel, ctx, invoke->pra));\n\tPERF_END);\n\tif (err)\n\t\tgoto bail;\n bail:\n\tif (ctx && interrupted == -ERESTARTSYS)\n\t\tcontext_save_interrupted(ctx);\n\telse if (ctx)\n\t\tcontext_free(ctx);\n\tif (fl->ssrcount != fl->apps->channel[cid].ssrcount)\n\t\terr = ECONNRESET;\n\n\tif (fl->profile && !interrupted) {\n\t\tif (invoke->handle != FASTRPC_STATIC_HANDLE_LISTENER) {\n\t\t\tint64_t *count = GET_COUNTER(perf_counter, PERF_INVOKE);\n\n\t\t\tif (count)\n\t\t\t\t*count += getnstimediff(&invoket);\n\t\t}\n\t\tif (invoke->handle > FASTRPC_STATIC_HANDLE_MAX) {\n\t\t\tint64_t *count = GET_COUNTER(perf_counter, PERF_COUNT);\n\n\t\t\tif (count)\n\t\t\t\t*count = *count+1;\n\t\t}\n\t}\n\treturn err;\n}'
p92
s(g79
S'fastrpc_init_process'
p93
tp94
S'static int fastrpc_init_process(struct fastrpc_file *fl,\n\t\t\t\tstruct fastrpc_ioctl_init_attrs *uproc)\n{\n\tint err = 0;\n\tstruct fastrpc_apps *me = &gfa;\n\tstruct fastrpc_ioctl_invoke_crc ioctl;\n\tstruct fastrpc_ioctl_init *init = &uproc->init;\n\tstruct smq_phy_page pages[1];\n\tstruct fastrpc_mmap *file = NULL, *mem = NULL;\n\tstruct fastrpc_buf *imem = NULL;\n\tunsigned long imem_dma_attr = 0;\n\tchar *proc_name = NULL;\n\n\tVERIFY(err, 0 == (err = fastrpc_channel_open(fl)));\n\tif (err)\n\t\tgoto bail;\n\tif (init->flags == FASTRPC_INIT_ATTACH ||\n\t\t\tinit->flags == FASTRPC_INIT_ATTACH_SENSORS) {\n\t\tremote_arg_t ra[1];\n\t\tint tgid = fl->tgid;\n\n\t\tra[0].buf.pv = (void *)&tgid;\n\t\tra[0].buf.len = sizeof(tgid);\n\t\tioctl.inv.handle = FASTRPC_STATIC_HANDLE_KERNEL;\n\t\tioctl.inv.sc = REMOTE_SCALARS_MAKE(0, 1, 0);\n\t\tioctl.inv.pra = ra;\n\t\tioctl.fds = NULL;\n\t\tioctl.attrs = NULL;\n\t\tioctl.crc = NULL;\n\t\tif (init->flags == FASTRPC_INIT_ATTACH)\n\t\t\tfl->pd = 0;\n\t\telse if (init->flags == FASTRPC_INIT_ATTACH_SENSORS) {\n\t\t\tfl->spdname = SENSORS_PDR_SERVICE_LOCATION_CLIENT_NAME;\n\t\t\tfl->pd = 2;\n\t\t}\n\t\tVERIFY(err, !(err = fastrpc_internal_invoke(fl,\n\t\t\tFASTRPC_MODE_PARALLEL, 1, &ioctl)));\n\t\tif (err)\n\t\t\tgoto bail;\n\t} else if (init->flags == FASTRPC_INIT_CREATE) {\n\t\tremote_arg_t ra[6];\n\t\tint fds[6];\n\t\tint mflags = 0;\n\t\tint memlen;\n\t\tstruct {\n\t\t\tint pgid;\n\t\t\tunsigned int namelen;\n\t\t\tunsigned int filelen;\n\t\t\tunsigned int pageslen;\n\t\t\tint attrs;\n\t\t\tint siglen;\n\t\t} inbuf;\n\n\t\tinbuf.pgid = fl->tgid;\n\t\tinbuf.namelen = strlen(current->comm) + 1;\n\t\tinbuf.filelen = init->filelen;\n\t\tfl->pd = 1;\n\n\t\tVERIFY(err, access_ok(0, (void __user *)init->file,\n\t\t\tinit->filelen));\n\t\tif (err)\n\t\t\tgoto bail;\n\t\tif (init->filelen) {\n\t\t\tmutex_lock(&fl->fl_map_mutex);\n\t\t\tVERIFY(err, !fastrpc_mmap_create(fl, init->filefd, 0,\n\t\t\t\tinit->file, init->filelen, mflags, &file));\n\t\t\tmutex_unlock(&fl->fl_map_mutex);\n\t\t\tif (err)\n\t\t\t\tgoto bail;\n\t\t}\n\t\tinbuf.pageslen = 1;\n\n\t\tVERIFY(err, !init->mem);\n\t\tif (err) {\n\t\t\terr = -EINVAL;\n\t\t\tpr_err("adsprpc: %s: %s: ERROR: donated memory allocated in userspace\\n",\n\t\t\t\tcurrent->comm, __func__);\n\t\t\tgoto bail;\n\t\t}\n\t\tmemlen = ALIGN(max(1024*1024*3, (int)init->filelen * 4),\n\t\t\t\t\t\t1024*1024);\n\t\timem_dma_attr = DMA_ATTR_EXEC_MAPPING |\n\t\t\t\t\t\tDMA_ATTR_NO_KERNEL_MAPPING |\n\t\t\t\t\t\tDMA_ATTR_FORCE_NON_COHERENT;\n\t\terr = fastrpc_buf_alloc(fl, memlen, imem_dma_attr, 0, 0, &imem);\n\t\tif (err)\n\t\t\tgoto bail;\n\t\tfl->init_mem = imem;\n\n\t\tinbuf.pageslen = 1;\n\t\tra[0].buf.pv = (void *)&inbuf;\n\t\tra[0].buf.len = sizeof(inbuf);\n\t\tfds[0] = 0;\n\n\t\tra[1].buf.pv = (void *)current->comm;\n\t\tra[1].buf.len = inbuf.namelen;\n\t\tfds[1] = 0;\n\n\t\tra[2].buf.pv = (void *)init->file;\n\t\tra[2].buf.len = inbuf.filelen;\n\t\tfds[2] = init->filefd;\n\n\t\tpages[0].addr = imem->phys;\n\t\tpages[0].size = imem->size;\n\t\tra[3].buf.pv = (void *)pages;\n\t\tra[3].buf.len = 1 * sizeof(*pages);\n\t\tfds[3] = 0;\n\n\t\tinbuf.attrs = uproc->attrs;\n\t\tra[4].buf.pv = (void *)&(inbuf.attrs);\n\t\tra[4].buf.len = sizeof(inbuf.attrs);\n\t\tfds[4] = 0;\n\n\t\tinbuf.siglen = uproc->siglen;\n\t\tra[5].buf.pv = (void *)&(inbuf.siglen);\n\t\tra[5].buf.len = sizeof(inbuf.siglen);\n\t\tfds[5] = 0;\n\n\t\tioctl.inv.handle = FASTRPC_STATIC_HANDLE_KERNEL;\n\t\tioctl.inv.sc = REMOTE_SCALARS_MAKE(6, 4, 0);\n\t\tif (uproc->attrs)\n\t\t\tioctl.inv.sc = REMOTE_SCALARS_MAKE(7, 6, 0);\n\t\tioctl.inv.pra = ra;\n\t\tioctl.fds = fds;\n\t\tioctl.attrs = NULL;\n\t\tioctl.crc = NULL;\n\t\tVERIFY(err, !(err = fastrpc_internal_invoke(fl,\n\t\t\tFASTRPC_MODE_PARALLEL, 1, &ioctl)));\n\t\tif (err)\n\t\t\tgoto bail;\n\t} else if (init->flags == FASTRPC_INIT_CREATE_STATIC) {\n\t\tremote_arg_t ra[3];\n\t\tuint64_t phys = 0;\n\t\tsize_t size = 0;\n\t\tint fds[3];\n\t\tstruct {\n\t\t\tint pgid;\n\t\t\tunsigned int namelen;\n\t\t\tunsigned int pageslen;\n\t\t} inbuf;\n\n\t\tif (!init->filelen)\n\t\t\tgoto bail;\n\n\t\tproc_name = kzalloc(init->filelen, GFP_KERNEL);\n\t\tVERIFY(err, !IS_ERR_OR_NULL(proc_name));\n\t\tif (err)\n\t\t\tgoto bail;\n\t\tVERIFY(err, 0 == copy_from_user((void *)proc_name,\n\t\t\t(void __user *)init->file, init->filelen));\n\t\tif (err)\n\t\t\tgoto bail;\n\n\t\tfl->pd = 1;\n\t\tinbuf.pgid = current->tgid;\n\t\tinbuf.namelen = init->filelen;\n\t\tinbuf.pageslen = 0;\n\n\t\tif (!strcmp(proc_name, "audiopd")) {\n\t\t\tfl->spdname = AUDIO_PDR_SERVICE_LOCATION_CLIENT_NAME;\n\t\t\tVERIFY(err, !fastrpc_mmap_remove_pdr(fl));\n\t\t\tif (err)\n\t\t\t\tgoto bail;\n\t\t}\n\n\t\tif (!me->staticpd_flags) {\n\t\t\tinbuf.pageslen = 1;\n\t\t\tmutex_lock(&fl->fl_map_mutex);\n\t\t\tVERIFY(err, !fastrpc_mmap_create(fl, -1, 0, init->mem,\n\t\t\t\t init->memlen, ADSP_MMAP_REMOTE_HEAP_ADDR,\n\t\t\t\t &mem));\n\t\t\tmutex_unlock(&fl->fl_map_mutex);\n\t\t\tif (err)\n\t\t\t\tgoto bail;\n\t\t\tphys = mem->phys;\n\t\t\tsize = mem->size;\n\t\t\tVERIFY(err, !hyp_assign_phys(phys, (uint64_t)size,\n\t\t\t\thlosvm, 1, me->channel[fl->cid].rhvm.vmid,\n\t\t\t\tme->channel[fl->cid].rhvm.vmperm,\n\t\t\t\tme->channel[fl->cid].rhvm.vmcount));\n\t\t\tif (err) {\n\t\t\t\tpr_err("ADSPRPC: hyp_assign_phys fail err %d",\n\t\t\t\t\t\t\t err);\n\t\t\t\tpr_err("map->phys %llx, map->size %d\\n",\n\t\t\t\t\t\t\t phys, (int)size);\n\t\t\t\tgoto bail;\n\t\t\t}\n\t\t\tme->staticpd_flags = 1;\n\t\t}\n\n\t\tra[0].buf.pv = (void *)&inbuf;\n\t\tra[0].buf.len = sizeof(inbuf);\n\t\tfds[0] = 0;\n\n\t\tra[1].buf.pv = (void *)proc_name;\n\t\tra[1].buf.len = inbuf.namelen;\n\t\tfds[1] = 0;\n\n\t\tpages[0].addr = phys;\n\t\tpages[0].size = size;\n\n\t\tra[2].buf.pv = (void *)pages;\n\t\tra[2].buf.len = sizeof(*pages);\n\t\tfds[2] = 0;\n\t\tioctl.inv.handle = FASTRPC_STATIC_HANDLE_KERNEL;\n\n\t\tioctl.inv.sc = REMOTE_SCALARS_MAKE(8, 3, 0);\n\t\tioctl.inv.pra = ra;\n\t\tioctl.fds = NULL;\n\t\tioctl.attrs = NULL;\n\t\tioctl.crc = NULL;\n\t\tVERIFY(err, !(err = fastrpc_internal_invoke(fl,\n\t\t\tFASTRPC_MODE_PARALLEL, 1, &ioctl)));\n\t\tif (err)\n\t\t\tgoto bail;\n\t} else {\n\t\terr = -ENOTTY;\n\t}\nbail:\n\tkfree(proc_name);\n\tif (err && (init->flags == FASTRPC_INIT_CREATE_STATIC))\n\t\tme->staticpd_flags = 0;\n\tif (mem && err) {\n\t\tif (mem->flags == ADSP_MMAP_REMOTE_HEAP_ADDR)\n\t\t\thyp_assign_phys(mem->phys, (uint64_t)mem->size,\n\t\t\t\t\tme->channel[fl->cid].rhvm.vmid,\n\t\t\t\t\tme->channel[fl->cid].rhvm.vmcount,\n\t\t\t\t\thlosvm, hlosvmperm, 1);\n\t\tmutex_lock(&fl->fl_map_mutex);\n\t\tfastrpc_mmap_free(mem, 0);\n\t\tmutex_unlock(&fl->fl_map_mutex);\n\t}\n\tif (file) {\n\t\tmutex_lock(&fl->fl_map_mutex);\n\t\tfastrpc_mmap_free(file, 0);\n\t\tmutex_unlock(&fl->fl_map_mutex);\n\t}\n\treturn err;\n}'
p95
s(g79
S'fastrpc_release_current_dsp_process'
p96
tp97
S'static int fastrpc_release_current_dsp_process(struct fastrpc_file *fl)\n{\n\tint err = 0;\n\tstruct fastrpc_ioctl_invoke_crc ioctl;\n\tremote_arg_t ra[1];\n\tint tgid = 0;\n\n\tVERIFY(err, fl->cid >= 0 && fl->cid < NUM_CHANNELS);\n\tif (err)\n\t\tgoto bail;\n\tVERIFY(err, fl->apps->channel[fl->cid].chan != NULL);\n\tif (err)\n\t\tgoto bail;\n\ttgid = fl->tgid;\n\tra[0].buf.pv = (void *)&tgid;\n\tra[0].buf.len = sizeof(tgid);\n\tioctl.inv.handle = FASTRPC_STATIC_HANDLE_KERNEL;\n\tioctl.inv.sc = REMOTE_SCALARS_MAKE(1, 1, 0);\n\tioctl.inv.pra = ra;\n\tioctl.fds = NULL;\n\tioctl.attrs = NULL;\n\tioctl.crc = NULL;\n\tVERIFY(err, 0 == (err = fastrpc_internal_invoke(fl,\n\t\tFASTRPC_MODE_PARALLEL, 1, &ioctl)));\nbail:\n\treturn err;\n}'
p98
s(g79
S'fastrpc_munmap_on_dsp'
p99
tp100
S'static int fastrpc_munmap_on_dsp(struct fastrpc_file *fl, uintptr_t raddr,\n\t\t\t\tuint64_t phys, size_t size, uint32_t flags)\n{\n\tstruct fastrpc_ioctl_invoke_crc ioctl;\n\tremote_arg_t ra[1];\n\tint err = 0;\n\tstruct {\n\t\tint pid;\n\t\tuintptr_t vaddrout;\n\t\tsize_t size;\n\t} inargs;\n\n\tinargs.pid = fl->tgid;\n\tinargs.size = size;\n\tinargs.vaddrout = raddr;\n\tra[0].buf.pv = (void *)&inargs;\n\tra[0].buf.len = sizeof(inargs);\n\n\tioctl.inv.handle = FASTRPC_STATIC_HANDLE_KERNEL;\n\tif (fl->apps->compat)\n\t\tioctl.inv.sc = REMOTE_SCALARS_MAKE(5, 1, 0);\n\telse\n\t\tioctl.inv.sc = REMOTE_SCALARS_MAKE(3, 1, 0);\n\tioctl.inv.pra = ra;\n\tioctl.fds = NULL;\n\tioctl.attrs = NULL;\n\tioctl.crc = NULL;\n\tVERIFY(err, 0 == (err = fastrpc_internal_invoke(fl,\n\t\tFASTRPC_MODE_PARALLEL, 1, &ioctl)));\n\tif (err)\n\t\tgoto bail;\n\tif (flags == ADSP_MMAP_HEAP_ADDR ||\n\t\t\t\tflags == ADSP_MMAP_REMOTE_HEAP_ADDR) {\n\t\tVERIFY(err, !fastrpc_munmap_on_dsp_rh(fl, phys, size, flags));\n\t\tif (err)\n\t\t\tgoto bail;\n\t}\nbail:\n\treturn err;\n}'
p101
ssS'7dca8885b04d'
p102
(dp103
(g79
g85
tp104
S'static int fastrpc_munmap_on_dsp_rh(struct fastrpc_file *fl, uint64_t phys,\n\t\t\t\t\t\tsize_t size, uint32_t flags)\n{\n\tint err = 0;\n\tstruct fastrpc_apps *me = &gfa;\n\tint tgid = 0;\n\tint destVM[1] = {VMID_HLOS};\n\tint destVMperm[1] = {PERM_READ | PERM_WRITE | PERM_EXEC};\n\n\tif (flags == ADSP_MMAP_HEAP_ADDR) {\n\t\tstruct fastrpc_ioctl_invoke_crc ioctl;\n\t\tstruct scm_desc desc = {0};\n\t\tremote_arg_t ra[2];\n\n\t\tstruct {\n\t\t\tuint8_t skey;\n\t\t} routargs;\n\n\t\tif (fl == NULL)\n\t\t\tgoto bail;\n\t\ttgid = fl->tgid;\n\t\tra[0].buf.pv = (void *)&tgid;\n\t\tra[0].buf.len = sizeof(tgid);\n\t\tra[1].buf.pv = (void *)&routargs;\n\t\tra[1].buf.len = sizeof(routargs);\n\n\t\tioctl.inv.handle = FASTRPC_STATIC_HANDLE_KERNEL;\n\t\tioctl.inv.sc = REMOTE_SCALARS_MAKE(9, 1, 1);\n\t\tioctl.inv.pra = ra;\n\t\tioctl.fds = NULL;\n\t\tioctl.attrs = NULL;\n\t\tioctl.crc = NULL;\n\n\n\t\tVERIFY(err, 0 == (err = fastrpc_internal_invoke(fl,\n\t\t\t\tFASTRPC_MODE_PARALLEL, 1, &ioctl)));\n\t\tif (err == AEE_EUNSUPPORTED) {\n\t\t\tremote_arg_t ra[1];\n\n\t\t\tpr_warn("ADSPRPC:Failed to get security key with updated remote call, falling back to older method");\n\t\t\tra[0].buf.pv = (void *)&routargs;\n\t\t\tra[0].buf.len = sizeof(routargs);\n\t\t\tioctl.inv.sc = REMOTE_SCALARS_MAKE(7, 0, 1);\n\t\t\tioctl.inv.pra = ra;\n\t\t\tVERIFY(err, 0 == (err = fastrpc_internal_invoke(fl,\n\t\t\t\tFASTRPC_MODE_PARALLEL, 1, &ioctl)));\n\t\t}\n\t\tif (err)\n\t\t\tgoto bail;\n\n\t\tdesc.args[0] = TZ_PIL_AUTH_QDSP6_PROC;\n\t\tdesc.args[1] = phys;\n\t\tdesc.args[2] = size;\n\t\tdesc.args[3] = routargs.skey;\n\t\tdesc.arginfo = SCM_ARGS(4);\n\t\terr = scm_call2(SCM_SIP_FNID(SCM_SVC_PIL,\n\t\t\tTZ_PIL_CLEAR_PROTECT_MEM_SUBSYS_ID), &desc);\n\t} else if (flags == ADSP_MMAP_REMOTE_HEAP_ADDR) {\n\t\tVERIFY(err, !hyp_assign_phys(phys, (uint64_t)size,\n\t\t\t\t\tme->channel[fl->cid].rhvm.vmid,\n\t\t\t\t\tme->channel[fl->cid].rhvm.vmcount,\n\t\t\t\t\tdestVM, destVMperm, 1));\n\t\tif (err)\n\t\t\tgoto bail;\n\t}\n\nbail:\n\treturn err;\n}'
p105
ssS'd6d7f6f8d00c'
p106
(dp107
(g79
g80
tp108
S'static int fastrpc_internal_invoke(struct fastrpc_file *fl, uint32_t mode,\n\t\t\t\t   uint32_t kernel,\n\t\t\t\t   struct fastrpc_ioctl_invoke_crc *inv)\n{\n\tstruct smq_invoke_ctx *ctx = NULL;\n\tstruct fastrpc_ioctl_invoke *invoke = &inv->inv;\n\tint err = 0, cid = -1, interrupted = 0;\n\tstruct timespec invoket = {0};\n\tint64_t *perf_counter = NULL;\n\n\tcid = fl->cid;\n\tVERIFY(err, cid >= ADSP_DOMAIN_ID && cid < NUM_CHANNELS);\n\tif (err) {\n\t\terr = -ECHRNG;\n\t\tgoto bail;\n\t}\n\tVERIFY(err, fl->sctx != NULL);\n\tif (err) {\n\t\terr = -EBADR;\n\t\tgoto bail;\n\t}\n\tperf_counter = getperfcounter(fl, PERF_COUNT);\n\n\tif (fl->profile)\n\t\tgetnstimeofday(&invoket);\n\n\tif (!kernel) {\n\t\tVERIFY(err, invoke->handle != FASTRPC_STATIC_HANDLE_KERNEL);\n\t\tif (err) {\n\t\t\tpr_err("adsprpc: ERROR: %s: user application %s trying to send a kernel RPC message to channel %d",\n\t\t\t\t__func__, current->comm, cid);\n\t\t\tgoto bail;\n\t\t}\n\t}\n\n\tif (!kernel) {\n\t\tVERIFY(err, 0 == context_restore_interrupted(fl, inv,\n\t\t\t\t\t\t\t\t&ctx));\n\t\tif (err)\n\t\t\tgoto bail;\n\t\tif (fl->sctx->smmu.faults)\n\t\t\terr = FASTRPC_ENOSUCH;\n\t\tif (err)\n\t\t\tgoto bail;\n\t\tif (ctx)\n\t\t\tgoto wait;\n\t}\n\n\tVERIFY(err, 0 == context_alloc(fl, kernel, inv, &ctx));\n\tif (err)\n\t\tgoto bail;\n\n\tif (REMOTE_SCALARS_LENGTH(ctx->sc)) {\n\t\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_GETARGS),\n\t\tVERIFY(err, 0 == get_args(kernel, ctx));\n\t\tPERF_END);\n\t\tif (err)\n\t\t\tgoto bail;\n\t}\n\n\tif (!fl->sctx->smmu.coherent) {\n\t\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_INVARGS),\n\t\tinv_args_pre(ctx);\n\t\tPERF_END);\n\t}\n\n\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_LINK),\n\tVERIFY(err, 0 == fastrpc_invoke_send(ctx, kernel, invoke->handle));\n\tPERF_END);\n\n\tif (err)\n\t\tgoto bail;\n wait:\n\tif (kernel)\n\t\twait_for_completion(&ctx->work);\n\telse {\n\t\tinterrupted = wait_for_completion_interruptible(&ctx->work);\n\t\tVERIFY(err, 0 == (err = interrupted));\n\t\tif (err)\n\t\t\tgoto bail;\n\t}\n\tif (ctx->handle)\n\t\tglink_rx_done(ctx->handle, ctx->ptr, true);\n\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_INVARGS),\n\tif (!fl->sctx->smmu.coherent)\n\t\tinv_args(ctx);\n\tPERF_END);\n\n\tVERIFY(err, 0 == (err = ctx->retval));\n\tif (err)\n\t\tgoto bail;\n\n\tPERF(fl->profile, GET_COUNTER(perf_counter, PERF_PUTARGS),\n\tVERIFY(err, 0 == put_args(kernel, ctx, invoke->pra));\n\tPERF_END);\n\tif (err)\n\t\tgoto bail;\n bail:\n\tif (ctx && interrupted == -ERESTARTSYS)\n\t\tcontext_save_interrupted(ctx);\n\telse if (ctx)\n\t\tcontext_free(ctx);\n\tif (fl->ssrcount != fl->apps->channel[cid].ssrcount)\n\t\terr = ECONNRESET;\n\n\tif (fl->profile && !interrupted) {\n\t\tif (invoke->handle != FASTRPC_STATIC_HANDLE_LISTENER) {\n\t\t\tint64_t *count = GET_COUNTER(perf_counter, PERF_INVOKE);\n\n\t\t\tif (count)\n\t\t\t\t*count += getnstimediff(&invoket);\n\t\t}\n\t\tif (invoke->handle > FASTRPC_STATIC_HANDLE_MAX) {\n\t\t\tint64_t *count = GET_COUNTER(perf_counter, PERF_COUNT);\n\n\t\t\tif (count)\n\t\t\t\t*count = *count+1;\n\t\t}\n\t}\n\treturn err;\n}'
p109
ssS'0c755962c9cc'
p110
(dp111
(g79
g85
tp112
S'static int fastrpc_munmap_on_dsp_rh(struct fastrpc_file *fl, uint64_t phys,\n\t\t\t\t\t\tsize_t size, uint32_t flags)\n{\n\tint err = 0;\n\tstruct fastrpc_apps *me = &gfa;\n\tint tgid = 0;\n\tint destVM[1] = {VMID_HLOS};\n\tint destVMperm[1] = {PERM_READ | PERM_WRITE | PERM_EXEC};\n\n\tif (flags == ADSP_MMAP_HEAP_ADDR) {\n\t\tstruct fastrpc_ioctl_invoke_crc ioctl;\n\t\tstruct scm_desc desc = {0};\n\t\tremote_arg_t ra[2];\n\n\t\tstruct {\n\t\t\tuint8_t skey;\n\t\t} routargs;\n\n\t\tif (fl == NULL)\n\t\t\tgoto bail;\n\t\ttgid = fl->tgid;\n\t\tra[0].buf.pv = (void *)&tgid;\n\t\tra[0].buf.len = sizeof(tgid);\n\t\tra[1].buf.pv = (void *)&routargs;\n\t\tra[1].buf.len = sizeof(routargs);\n\n\t\tioctl.inv.handle = FASTRPC_STATIC_HANDLE_KERNEL;\n\t\tioctl.inv.sc = REMOTE_SCALARS_MAKE(9, 1, 1);\n\t\tioctl.inv.pra = ra;\n\t\tioctl.fds = NULL;\n\t\tioctl.attrs = NULL;\n\t\tioctl.crc = NULL;\n\n\n\t\tVERIFY(err, 0 == (err = fastrpc_internal_invoke(fl,\n\t\t\t\tFASTRPC_MODE_PARALLEL, 1, &ioctl)));\n\t\tif (err)\n\t\t\tgoto bail;\n\t\tdesc.args[0] = TZ_PIL_AUTH_QDSP6_PROC;\n\t\tdesc.args[1] = phys;\n\t\tdesc.args[2] = size;\n\t\tdesc.args[3] = routargs.skey;\n\t\tdesc.arginfo = SCM_ARGS(4);\n\t\terr = scm_call2(SCM_SIP_FNID(SCM_SVC_PIL,\n\t\t\tTZ_PIL_CLEAR_PROTECT_MEM_SUBSYS_ID), &desc);\n\t} else if (flags == ADSP_MMAP_REMOTE_HEAP_ADDR) {\n\t\tVERIFY(err, !hyp_assign_phys(phys, (uint64_t)size,\n\t\t\t\t\tme->channel[fl->cid].rhvm.vmid,\n\t\t\t\t\tme->channel[fl->cid].rhvm.vmcount,\n\t\t\t\t\tdestVM, destVMperm, 1));\n\t\tif (err)\n\t\t\tgoto bail;\n\t}\n\nbail:\n\treturn err;\n}'
p113
sssg15
S'6c6aaf4e8330'
p114
ssS'CVE-2019-2323'
p115
(dp116
g3
(dp117
S'de6abb23dc05'
p118
(dp119
(S'drivers/crypto/msm/ice.c'
p120
S'qcom_ice_setup_ice_hw'
p121
tp122
S'int qcom_ice_setup_ice_hw(const char *storage_type, int enable)\n{\n\tint ret = -1;\n\tstruct ice_device *ice_dev = NULL;\n\n\tice_dev = get_ice_device_from_storage_type(storage_type);\n\tif (ice_dev == ERR_PTR(-EPROBE_DEFER))\n\t\treturn -EPROBE_DEFER;\n\n\tif (!ice_dev || (ice_dev->is_ice_enabled == false))\n\t\treturn ret;\n\n\tif (enable)\n\t\treturn enable_ice_setup(ice_dev);\n\telse\n\t\treturn disable_ice_setup(ice_dev);\n}'
p123
s(g120
S'get_ice_device_from_storage_type'
p124
tp125
S''
p126
sssg15
S'2dc9dcdbb971'
p127
ssS'CVE-2019-2330'
p128
(dp129
g3
(dp130
S'19a8101c2309'
p131
(dp132
(S'drivers/staging/android/ion/ion_system_heap.c'
p133
S'alloc_from_pool_preferred'
p134
tp135
S'static struct page_info *alloc_from_pool_preferred(\n\t\tstruct ion_system_heap *heap, struct ion_buffer *buffer,\n\t\tunsigned long size, unsigned int max_order)\n{\n\tstruct page *page;\n\tstruct page_info *info;\n\tint i;\n\n\tif (buffer->flags & ION_FLAG_POOL_FORCE_ALLOC)\n\t\tgoto force_alloc;\n\n\tinfo = kmalloc(sizeof(*info), GFP_KERNEL);\n\tif (!info)\n\t\treturn NULL;\n\n\tfor (i = 0; i < num_orders; i++) {\n\t\tif (size < order_to_size(orders[i]))\n\t\t\tcontinue;\n\t\tif (max_order < orders[i])\n\t\t\tcontinue;\n\n\t\tpage = alloc_from_secure_pool_order(heap, buffer, orders[i]);\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\tinfo->page = page;\n\t\tinfo->order = orders[i];\n\t\tinfo->from_pool = true;\n\t\tINIT_LIST_HEAD(&info->list);\n\t\treturn info;\n\t}\n\n\tpage = split_page_from_secure_pool(heap, buffer);\n\tif (page) {\n\t\tinfo->page = page;\n\t\tinfo->order = 0;\n\t\tinfo->from_pool = true;\n\t\tINIT_LIST_HEAD(&info->list);\n\t\treturn info;\n\t}\n\n\tkfree(info);\nforce_alloc:\n\treturn alloc_largest_available(heap, buffer, size, max_order);\n}'
p136
sssg15
S'1fb9158725c8'
p137
ssS'CVE-2019-2283'
p138
(dp139
g3
(dp140
S'2dc9dcdbb971'
p141
(dp142
(S'drivers/soc/qcom/glink_smem_native_xprt.c'
p143
S'fifo_read'
p144
tp145
S'static int fifo_read(struct edge_info *einfo, void *_data, int len)\n{\n\tvoid *ptr;\n\tvoid *ret;\n\tvoid *data = _data;\n\tint orig_len = len;\n\tuint32_t read_index = einfo->rx_ch_desc->read_index;\n\tuint32_t write_index = einfo->rx_ch_desc->write_index;\n\tuint32_t fifo_size = einfo->rx_fifo_size;\n\tuint32_t n;\n\n\tif (read_index >= fifo_size || write_index >= fifo_size)\n\t\treturn 0;\n\twhile (len) {\n\t\tptr = einfo->rx_fifo + read_index;\n\t\tif (read_index <= write_index)\n\t\t\tn = write_index - read_index;\n\t\telse\n\t\t\tn = fifo_size - read_index;\n\n\t\tif (n == 0)\n\t\t\tbreak;\n\t\tif (n > len)\n\t\t\tn = len;\n\n\t\tret = einfo->read_from_fifo(data, ptr, n);\n\t\tif (IS_ERR(ret))\n\t\t\treturn PTR_ERR(ret);\n\n\t\tdata += n;\n\t\tlen -= n;\n\t\tread_index += n;\n\t\tif (read_index >= fifo_size)\n\t\t\tread_index -= fifo_size;\n\t}\n\teinfo->rx_ch_desc->read_index = read_index;\n\n\treturn orig_len - len;\n}'
p146
s(g143
S'fifo_write_body'
p147
tp148
S'static int fifo_write_body(struct edge_info *einfo, const void *_data,\n\t\t\t\tint len, uint32_t *write_index)\n{\n\tvoid *ptr;\n\tvoid *ret;\n\tconst void *data = _data;\n\tuint32_t read_index = einfo->tx_ch_desc->read_index;\n\tuint32_t fifo_size = einfo->tx_fifo_size;\n\tuint32_t n;\n\n\tif (read_index >= fifo_size || *write_index >= fifo_size)\n\t\treturn 0;\n\twhile (len) {\n\t\tptr = einfo->tx_fifo + *write_index;\n\t\tif (*write_index < read_index) {\n\t\t\tn = read_index - *write_index - FIFO_FULL_RESERVE;\n\t\t} else {\n\t\t\tif (read_index < FIFO_FULL_RESERVE)\n\t\t\t\tn = fifo_size + read_index - *write_index -\n\t\t\t\t\t\t\tFIFO_FULL_RESERVE;\n\t\t\telse\n\t\t\t\tn = fifo_size - *write_index;\n\t\t}\n\n\t\tif (n == 0)\n\t\t\tbreak;\n\t\tif (n > len)\n\t\t\tn = len;\n\n\t\tret = einfo->write_to_fifo(ptr, data, n);\n\t\tif (IS_ERR(ret))\n\t\t\treturn PTR_ERR(ret);\n\n\t\tdata += n;\n\t\tlen -= n;\n\t\t*write_index += n;\n\t\tif (*write_index >= fifo_size)\n\t\t\t*write_index -= fifo_size;\n\t}\n\treturn len;\n}'
p149
ssS'70f432282130'
p150
(dp151
(g143
g144
tp152
S'static int fifo_read(struct edge_info *einfo, void *_data, int len)\n{\n\tvoid *ptr;\n\tvoid *ret;\n\tvoid *data = _data;\n\tint orig_len = len;\n\tuint32_t read_index = einfo->rx_ch_desc->read_index;\n\tuint32_t write_index = einfo->rx_ch_desc->write_index;\n\tuint32_t fifo_size = einfo->rx_fifo_size;\n\tuint32_t n;\n\n\tif (read_index >= fifo_size || write_index >= fifo_size) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn -EINVAL;\n\t}\n\twhile (len) {\n\t\tptr = einfo->rx_fifo + read_index;\n\t\tif (read_index <= write_index)\n\t\t\tn = write_index - read_index;\n\t\telse\n\t\t\tn = fifo_size - read_index;\n\n\t\tif (n == 0)\n\t\t\tbreak;\n\t\tif (n > len)\n\t\t\tn = len;\n\n\t\tret = einfo->read_from_fifo(data, ptr, n);\n\t\tif (IS_ERR(ret))\n\t\t\treturn PTR_ERR(ret);\n\n\t\tdata += n;\n\t\tlen -= n;\n\t\tread_index += n;\n\t\tif (read_index >= fifo_size)\n\t\t\tread_index -= fifo_size;\n\t}\n\teinfo->rx_ch_desc->read_index = read_index;\n\n\treturn orig_len - len;\n}'
p153
s(g143
g147
tp154
S'static int fifo_write_body(struct edge_info *einfo, const void *_data,\n\t\t\t\tint len, uint32_t *write_index)\n{\n\tvoid *ptr;\n\tvoid *ret;\n\tconst void *data = _data;\n\tuint32_t read_index = einfo->tx_ch_desc->read_index;\n\tuint32_t fifo_size = einfo->tx_fifo_size;\n\tuint32_t n;\n\n\tif (read_index >= fifo_size || *write_index >= fifo_size) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn -EINVAL;\n\t}\n\twhile (len) {\n\t\tptr = einfo->tx_fifo + *write_index;\n\t\tif (*write_index < read_index) {\n\t\t\tn = read_index - *write_index - FIFO_FULL_RESERVE;\n\t\t} else {\n\t\t\tif (read_index < FIFO_FULL_RESERVE)\n\t\t\t\tn = fifo_size + read_index - *write_index -\n\t\t\t\t\t\t\tFIFO_FULL_RESERVE;\n\t\t\telse\n\t\t\t\tn = fifo_size - *write_index;\n\t\t}\n\n\t\tif (n == 0)\n\t\t\tbreak;\n\t\tif (n > len)\n\t\t\tn = len;\n\n\t\tret = einfo->write_to_fifo(ptr, data, n);\n\t\tif (IS_ERR(ret))\n\t\t\treturn PTR_ERR(ret);\n\n\t\tdata += n;\n\t\tlen -= n;\n\t\t*write_index += n;\n\t\tif (*write_index >= fifo_size)\n\t\t\t*write_index -= fifo_size;\n\t}\n\treturn len;\n}'
p155
ssS'c36d54c34fef'
p156
(dp157
(g143
g144
tp158
S'static int fifo_read(struct edge_info *einfo, void *_data, int len)\n{\n\tvoid *ptr;\n\tvoid *ret;\n\tvoid *data = _data;\n\tint orig_len = len;\n\tuint32_t read_index = einfo->rx_ch_desc->read_index;\n\tuint32_t write_index = einfo->rx_ch_desc->write_index;\n\tuint32_t fifo_size = einfo->rx_fifo_size;\n\tuint32_t n;\n\n\tif (read_index > fifo_size && write_index > fifo_size)\n\t\treturn 0;\n\twhile (len) {\n\t\tptr = einfo->rx_fifo + read_index;\n\t\tif (read_index <= write_index)\n\t\t\tn = write_index - read_index;\n\t\telse\n\t\t\tn = fifo_size - read_index;\n\n\t\tif (n == 0)\n\t\t\tbreak;\n\t\tif (n > len)\n\t\t\tn = len;\n\n\t\tret = einfo->read_from_fifo(data, ptr, n);\n\t\tif (IS_ERR(ret))\n\t\t\treturn PTR_ERR(ret);\n\n\t\tdata += n;\n\t\tlen -= n;\n\t\tread_index += n;\n\t\tif (read_index >= fifo_size)\n\t\t\tread_index -= fifo_size;\n\t}\n\teinfo->rx_ch_desc->read_index = read_index;\n\n\treturn orig_len - len;\n}'
p159
s(g143
g147
tp160
S'static int fifo_write_body(struct edge_info *einfo, const void *_data,\n\t\t\t\tint len, uint32_t *write_index)\n{\n\tvoid *ptr;\n\tvoid *ret;\n\tconst void *data = _data;\n\tuint32_t read_index = einfo->tx_ch_desc->read_index;\n\tuint32_t fifo_size = einfo->tx_fifo_size;\n\tuint32_t n;\n\n\tif (read_index > fifo_size && *write_index > fifo_size)\n\t\treturn 0;\n\twhile (len) {\n\t\tptr = einfo->tx_fifo + *write_index;\n\t\tif (*write_index < read_index) {\n\t\t\tn = read_index - *write_index - FIFO_FULL_RESERVE;\n\t\t} else {\n\t\t\tif (read_index < FIFO_FULL_RESERVE)\n\t\t\t\tn = fifo_size + read_index - *write_index -\n\t\t\t\t\t\t\tFIFO_FULL_RESERVE;\n\t\t\telse\n\t\t\t\tn = fifo_size - *write_index;\n\t\t}\n\n\t\tif (n == 0)\n\t\t\tbreak;\n\t\tif (n > len)\n\t\t\tn = len;\n\n\t\tret = einfo->write_to_fifo(ptr, data, n);\n\t\tif (IS_ERR(ret))\n\t\t\treturn PTR_ERR(ret);\n\n\t\tdata += n;\n\t\tlen -= n;\n\t\t*write_index += n;\n\t\tif (*write_index >= fifo_size)\n\t\t\t*write_index -= fifo_size;\n\t}\n\treturn len;\n}'
p161
sssg15
S'a284d4c9bbea'
p162
ssS'CVE-2019-2260'
p163
(dp164
g3
(dp165
S'c36d54c34fef'
p166
(dp167
(S'kernel/events/core.c'
p168
S'__perf_event_release_kernel'
p169
tp170
S"static int __perf_event_release_kernel(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *child, *tmp;\n\n#if defined CONFIG_HOTPLUG_CPU || defined CONFIG_KEXEC_CORE\n\tif (event->cpu != -1) {\n\t\tspin_lock(&dormant_event_list_lock);\n\t\tif (event->state == PERF_EVENT_STATE_DORMANT)\n\t\t\tlist_del(&event->dormant_event_entry);\n\t\tspin_unlock(&dormant_event_list_lock);\n\t}\n#endif\n\n\t/*\n\t * If we got here through err_file: fput(event_file); we will not have\n\t * attached to a context yet.\n\t */\n\tif (!ctx) {\n\t\tWARN_ON_ONCE(event->attach_state &\n\t\t\t\t(PERF_ATTACH_CONTEXT|PERF_ATTACH_GROUP));\n\t\tgoto no_ctx;\n\t}\n\n\tif (!is_kernel_event(event)) {\n\t\tperf_remove_from_owner(event);\n\t}\n\n\tctx = perf_event_ctx_lock(event);\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tperf_remove_from_context(event, DETACH_GROUP);\n\n\tif (perf_event_delete_kernel_shared(event) > 0) {\n\t\tperf_event__state_init(event);\n\t\tperf_install_in_context(ctx, event, event->cpu);\n\n\t\tperf_event_ctx_unlock(event, ctx);\n\n\t\tperf_event_enable(event);\n\n\t\treturn 0;\n\t}\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * Mark this even as STATE_DEAD, there is no external reference to it\n\t * anymore.\n\t *\n\t * Anybody acquiring event->child_mutex after the below loop _must_\n\t * also see this, most importantly inherit_event() which will avoid\n\t * placing more children on the list.\n\t *\n\t * Thus this guarantees that we will in fact observe and kill _ALL_\n\t * child events.\n\t */\n\tevent->state = PERF_EVENT_STATE_DEAD;\n\traw_spin_unlock_irq(&ctx->lock);\n\n\tperf_event_ctx_unlock(event, ctx);\n\nagain:\n\tmutex_lock(&event->child_mutex);\n\tlist_for_each_entry(child, &event->child_list, child_list) {\n\n\t\t/*\n\t\t * Cannot change, child events are not migrated, see the\n\t\t * comment with perf_event_ctx_lock_nested().\n\t\t */\n\t\tctx = lockless_dereference(child->ctx);\n\t\t/*\n\t\t * Since child_mutex nests inside ctx::mutex, we must jump\n\t\t * through hoops. We start by grabbing a reference on the ctx.\n\t\t *\n\t\t * Since the event cannot get freed while we hold the\n\t\t * child_mutex, the context must also exist and have a !0\n\t\t * reference count.\n\t\t */\n\t\tget_ctx(ctx);\n\n\t\t/*\n\t\t * Now that we have a ctx ref, we can drop child_mutex, and\n\t\t * acquire ctx::mutex without fear of it going away. Then we\n\t\t * can re-acquire child_mutex.\n\t\t */\n\t\tmutex_unlock(&event->child_mutex);\n\t\tmutex_lock(&ctx->mutex);\n\t\tmutex_lock(&event->child_mutex);\n\n\t\t/*\n\t\t * Now that we hold ctx::mutex and child_mutex, revalidate our\n\t\t * state, if child is still the first entry, it didn't get freed\n\t\t * and we can continue doing so.\n\t\t */\n\t\ttmp = list_first_entry_or_null(&event->child_list,\n\t\t\t\t\t       struct perf_event, child_list);\n\t\tif (tmp == child) {\n\t\t\tperf_remove_from_context(child, DETACH_GROUP);\n\t\t\tlist_del(&child->child_list);\n\t\t\tfree_event(child);\n\t\t\t/*\n\t\t\t * This matches the refcount bump in inherit_event();\n\t\t\t * this can't be the last reference.\n\t\t\t */\n\t\t\tput_event(event);\n\t\t}\n\n\t\tmutex_unlock(&event->child_mutex);\n\t\tmutex_unlock(&ctx->mutex);\n\t\tput_ctx(ctx);\n\t\tgoto again;\n\t}\n\tmutex_unlock(&event->child_mutex);\n\nno_ctx:\n\tput_event(event); /* Must be the 'last' reference */\n\treturn 0;\n}"
p171
s(g168
S'perf_event_exit_cpu'
p172
tp173
S'int perf_event_exit_cpu(unsigned int cpu)\n{\n\n\tmutex_lock(&pmus_lock);\n\tperf_event_exit_cpu_context(cpu);\n\tmutex_unlock(&pmus_lock);\n\treturn 0;\n}'
p174
ssS'f332617ebb03'
p175
(dp176
(g168
S'perf_event_zombie_cleanup'
p177
tp178
S'static void perf_event_zombie_cleanup(unsigned int cpu)\n{\n\tstruct perf_event *event, *tmp;\n\n\tspin_lock(&zombie_list_lock);\n\n\tlist_for_each_entry_safe(event, tmp, &zombie_list, zombie_entry) {\n\t\tif (event->cpu != cpu)\n\t\t\tcontinue;\n\n\t\tlist_del(&event->zombie_entry);\n\t\tspin_unlock(&zombie_list_lock);\n\n\t\t/*\n\t\t * The detachment of the event with the\n\t\t * PMU expects it to be in an active state\n\t\t */\n\t\tevent->state = PERF_EVENT_STATE_ACTIVE;\n\t\t__perf_event_release_kernel(event);\n\n\t\tspin_lock(&zombie_list_lock);\n\t}\n\n\tspin_unlock(&zombie_list_lock);\n}'
p179
s(g168
S'perf_event_release_kernel'
p180
tp181
S'int perf_event_release_kernel(struct perf_event *event)\n{\n\tint ret;\n\n\tmutex_lock(&pmus_lock);\n\tret = __perf_event_release_kernel(event);\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n}'
p182
s(g168
S'perf_event_start_swevents'
p183
tp184
S'static int perf_event_start_swevents(unsigned int cpu)\n{\n\tstruct perf_event_context *ctx;\n\tstruct pmu *pmu;\n\tstruct perf_event *event;\n\tint idx;\n\n\tmutex_lock(&pmus_lock);\n\tperf_event_zombie_cleanup(cpu);\n\n\tidx = srcu_read_lock(&pmus_srcu);\n\tlist_for_each_entry_rcu(pmu, &pmus, entry) {\n\t\tctx = &per_cpu_ptr(pmu->pmu_cpu_context, cpu)->ctx;\n\t\tmutex_lock(&ctx->mutex);\n\t\traw_spin_lock(&ctx->lock);\n\t\tlist_for_each_entry(event, &ctx->event_list, event_entry)\n\t\t\tcheck_hotplug_start_event(event);\n\t\traw_spin_unlock(&ctx->lock);\n\t\tmutex_unlock(&ctx->mutex);\n\t}\n\tsrcu_read_unlock(&pmus_srcu, idx);\n\tper_cpu(is_hotplugging, cpu) = false;\n\tmutex_unlock(&pmus_lock);\n\n\treturn 0;\n}'
p185
s(g168
g169
tp186
S"static int __perf_event_release_kernel(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *child, *tmp;\n\n\t/*\n\t * If the cpu associated to this event is offline, set the event as a\n\t *  zombie event. The cleanup of the cpu would be done if the CPU is\n\t *  back online.\n\t */\n#if defined CONFIG_HOTPLUG_CPU || defined CONFIG_KEXEC_CORE\n\tif (event->cpu != -1 && per_cpu(is_hotplugging, event->cpu)) {\n\t\tif (event->state == PERF_EVENT_STATE_ZOMBIE)\n\t\t\treturn 0;\n\n\t\tevent->state = PERF_EVENT_STATE_ZOMBIE;\n\n\t\tspin_lock(&zombie_list_lock);\n\t\tlist_add_tail(&event->zombie_entry, &zombie_list);\n\t\tspin_unlock(&zombie_list_lock);\n\n\t\treturn 0;\n\t}\n#endif\n\n\t/*\n\t * If we got here through err_file: fput(event_file); we will not have\n\t * attached to a context yet.\n\t */\n\tif (!ctx) {\n\t\tWARN_ON_ONCE(event->attach_state &\n\t\t\t\t(PERF_ATTACH_CONTEXT|PERF_ATTACH_GROUP));\n\t\tgoto no_ctx;\n\t}\n\n\tif (!is_kernel_event(event)) {\n\t\tperf_remove_from_owner(event);\n\t}\n\n\tctx = perf_event_ctx_lock(event);\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tperf_remove_from_context(event, DETACH_GROUP);\n\n\tif (perf_event_delete_kernel_shared(event) > 0) {\n\t\tperf_event__state_init(event);\n\t\tperf_install_in_context(ctx, event, event->cpu);\n\n\t\tperf_event_ctx_unlock(event, ctx);\n\n\t\tperf_event_enable(event);\n\n\t\treturn 0;\n\t}\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * Mark this even as STATE_DEAD, there is no external reference to it\n\t * anymore.\n\t *\n\t * Anybody acquiring event->child_mutex after the below loop _must_\n\t * also see this, most importantly inherit_event() which will avoid\n\t * placing more children on the list.\n\t *\n\t * Thus this guarantees that we will in fact observe and kill _ALL_\n\t * child events.\n\t */\n\tevent->state = PERF_EVENT_STATE_DEAD;\n\traw_spin_unlock_irq(&ctx->lock);\n\n\tperf_event_ctx_unlock(event, ctx);\n\nagain:\n\tmutex_lock(&event->child_mutex);\n\tlist_for_each_entry(child, &event->child_list, child_list) {\n\n\t\t/*\n\t\t * Cannot change, child events are not migrated, see the\n\t\t * comment with perf_event_ctx_lock_nested().\n\t\t */\n\t\tctx = lockless_dereference(child->ctx);\n\t\t/*\n\t\t * Since child_mutex nests inside ctx::mutex, we must jump\n\t\t * through hoops. We start by grabbing a reference on the ctx.\n\t\t *\n\t\t * Since the event cannot get freed while we hold the\n\t\t * child_mutex, the context must also exist and have a !0\n\t\t * reference count.\n\t\t */\n\t\tget_ctx(ctx);\n\n\t\t/*\n\t\t * Now that we have a ctx ref, we can drop child_mutex, and\n\t\t * acquire ctx::mutex without fear of it going away. Then we\n\t\t * can re-acquire child_mutex.\n\t\t */\n\t\tmutex_unlock(&event->child_mutex);\n\t\tmutex_lock(&ctx->mutex);\n\t\tmutex_lock(&event->child_mutex);\n\n\t\t/*\n\t\t * Now that we hold ctx::mutex and child_mutex, revalidate our\n\t\t * state, if child is still the first entry, it didn't get freed\n\t\t * and we can continue doing so.\n\t\t */\n\t\ttmp = list_first_entry_or_null(&event->child_list,\n\t\t\t\t\t       struct perf_event, child_list);\n\t\tif (tmp == child) {\n\t\t\tperf_remove_from_context(child, DETACH_GROUP);\n\t\t\tlist_del(&child->child_list);\n\t\t\tfree_event(child);\n\t\t\t/*\n\t\t\t * This matches the refcount bump in inherit_event();\n\t\t\t * this can't be the last reference.\n\t\t\t */\n\t\t\tput_event(event);\n\t\t}\n\n\t\tmutex_unlock(&event->child_mutex);\n\t\tmutex_unlock(&ctx->mutex);\n\t\tput_ctx(ctx);\n\t\tgoto again;\n\t}\n\tmutex_unlock(&event->child_mutex);\n\nno_ctx:\n\tput_event(event); /* Must be the 'last' reference */\n\treturn 0;\n}"
p187
s(g168
g172
tp188
S'int perf_event_exit_cpu(unsigned int cpu)\n{\n\tmutex_lock(&pmus_lock);\n\tper_cpu(is_hotplugging, cpu) = true;\n\tperf_event_exit_cpu_context(cpu);\n\tmutex_unlock(&pmus_lock);\n\treturn 0;\n}'
p189
sssg15
S'ad6d811a2827'
p190
ss.